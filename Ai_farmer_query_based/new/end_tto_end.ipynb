{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d988f6-1a38-446a-82ef-f104dbaff37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from transformers[torch]) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from transformers[torch]) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch>=2.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from transformers[torch]) (2.8.0)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.2->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.2->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.2->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.2->transformers[torch]) (2.1.3)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819dc1f-cc37-4d76-901e-776e846e4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq.csv\"})\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"questions\"], padding=True, truncation=True)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset = dataset.rename_column(\"answers\", \"labels\")\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Model\n",
    "num_labels = len(set(dataset[\"train\"][\"labels\"]))\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"src/models/intent_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac7f206-a70b-4ad6-a3ca-202c4d3bb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.5/1.1 MB 560.1 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.5/1.1 MB 560.1 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.5/1.1 MB 560.1 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 486.4 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 486.4 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 486.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 449.0 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A58E7662D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sentencepiece/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A58E742960>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sentencepiece/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A58E7417C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sentencepiece/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A58E75D280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sentencepiece/\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984c62a-f705-457e-8eec-00a987db4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq.csv\"})\n",
    "\n",
    "# Tokenizer & Model\n",
    "model_name = \"t5-small\"  # you can also try \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"questions\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(batch[\"answers\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True, remove_columns=[\"questions\", \"answers\"])\n",
    "dataset.set_format(type=\"torch\")\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"src/models/qa_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851feaeb-f126-4e86-bb88-7a40b8a852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============================\n",
    "# # 1. Install Required Libraries\n",
    "# # ===============================\n",
    "# !pip install transformers datasets sentence-transformers faiss-cpu torch pandas scikit-learn\n",
    "\n",
    "# # ===============================\n",
    "# # 2. Import Libraries\n",
    "# # ===============================\n",
    "# import pandas as pd\n",
    "# from datasets import load_dataset\n",
    "# from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "# import torch\n",
    "\n",
    "# # ===============================\n",
    "# # 3. Load and Prepare Data\n",
    "# # ===============================\n",
    "# # Load FAQ dataset (combine faq.csv and faq2.csv if both exist)\n",
    "# faq1 = pd.read_csv(r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq.csv\")\n",
    "# faq2 = pd.read_csv(r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq2.csv\")\n",
    "# faq = pd.concat([faq1, faq2]).dropna().reset_index(drop=True)\n",
    "\n",
    "# # Ensure labels are integers\n",
    "# faq[\"category\"] = faq[\"category\"].astype(\"category\")\n",
    "# faq[\"label\"] = faq[\"category\"].cat.codes\n",
    "\n",
    "# faq.to_csv(r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq_combined.csv\", index=False)\n",
    "\n",
    "# # Load into HuggingFace dataset\n",
    "# dataset = load_dataset(\"csv\", data_files={\"train\": r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq_combined.csv\"})\n",
    "\n",
    "# # ===============================\n",
    "# # 4. Tokenization\n",
    "# # ===============================\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# def tokenize(batch):\n",
    "#     return tokenizer(batch[\"questions\"], padding=True, truncation=True)\n",
    "\n",
    "# dataset = dataset.map(tokenize, batched=True)\n",
    "# dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "# dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# # ===============================\n",
    "# # 5. Train Intent Classifier\n",
    "# # ===============================\n",
    "# num_labels = len(set(faq[\"labels\"]))  # number of categories\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"src/models/intent_model\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_dir=\"logs\",\n",
    "#     logging_steps=50,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.save_model(\"src/models/intent_model\")\n",
    "\n",
    "# # ===============================\n",
    "# # 6. Build Retriever with FAISS\n",
    "# # ===============================\n",
    "# embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# faq_embeddings = embedder.encode(faq[\"question\"].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# index = faiss.IndexFlatL2(faq_embeddings.shape[1])\n",
    "# index.add(faq_embeddings)\n",
    "\n",
    "# def retrieve_answer(query, top_k=1):\n",
    "#     query_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "#     D, I = index.search(query_vec, k=top_k)\n",
    "#     return faq.iloc[I[0][0]][\"answer\"]\n",
    "\n",
    "# # ===============================\n",
    "# # 7. Connect Intent + Retriever\n",
    "# # ===============================\n",
    "# intent_classifier = pipeline(\"text-classification\", model=\"src/models/intent_model\", tokenizer=tokenizer)\n",
    "\n",
    "# def get_response(query):\n",
    "#     # Step 1: Detect intent\n",
    "#     intent = intent_classifier(query)[0][\"label\"]\n",
    "\n",
    "#     # Step 2: Retrieve best answer\n",
    "#     try:\n",
    "#         answer = retrieve_answer(query)\n",
    "#     except:\n",
    "#         answer = None\n",
    "\n",
    "#     # Step 3: Fallback\n",
    "#     if not answer:\n",
    "#         answer = \"Sorry, I donâ€™t know the exact answer. Please consult an expert.\"\n",
    "#     return f\"Intent: {intent}\\nAnswer: {answer}\"\n",
    "\n",
    "# # ===============================\n",
    "# # 8. Test the System\n",
    "# # ===============================\n",
    "# print(get_response(\"How to treat wheat rust?\"))\n",
    "# print(get_response(\"Best fertilizer for rice?\"))\n",
    "# print(get_response(\"What is the ideal soil for sugarcane?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53513103-c6b7-4668-9389-bc51d036b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538f678b-ee9d-4912-b15c-733c7f8c50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['questions', 'answersr\"C:\\\\Users\\\\Lenovo\\\\Ai_farmer_query_based\\\\data']\n",
      "Sample row: {'questions': 'What is the best time to plant rice?', 'answersr\"C:\\\\Users\\\\Lenovo\\\\Ai_farmer_query_based\\\\data': 'The best time to plant rice depends on your region, but generally it is during the monsoon season between June and July.'}\n"
     ]
    }
   ],
   "source": [
    "# # Load your CSV (faq.csv or faq2.csv)\n",
    "# dataset = load_dataset(\"csv\", data_files=r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq.csv\")[\"train\"]\n",
    "\n",
    "# print(\"Columns:\", dataset.column_names)\n",
    "# print(\"Sample row:\", dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc391b4a-6a6d-4f74-94d6-3b8122d62321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load both FAQ files\n",
    "faq1 = load_dataset(\"csv\", data_files=r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq.csv\")[\"train\"]\n",
    "faq2 = load_dataset(\"csv\", data_files=r\"C:\\Users\\Lenovo\\Ai_farmer_query_based\\data\\faq2.csv\")[\"train\"]\n",
    "\n",
    "# Merge them into one dataset\n",
    "dataset = concatenate_datasets([faq1, faq2])\n",
    "\n",
    "print(\"Combined dataset size:\", len(dataset))\n",
    "print(\"Columns:\", dataset.column_names)\n",
    "print(\"Sample row:\", dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203c1a3-bc93-4b74-a47d-7e67ca3ea84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pretrained model for embeddings\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Encode all FAQ questions\n",
    "# question_embeddings = model.encode(dataset[\"questions\"], convert_to_numpy=True)\n",
    "\n",
    "# # Create FAISS index\n",
    "# index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "# index.add(question_embeddings)\n",
    "\n",
    "# print(\"FAISS index built with\", index.ntotal, \"questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5ec94-6798-4376-bccb-80e52b86b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Use pretrained embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode questions\n",
    "question_embeddings = model.encode(dataset[\"questions\"], convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "print(\"FAISS index built with\", index.ntotal, \"questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f1878-6ed3-44ad-8b09-776ef995a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"answersr\\\"C:\\\\Users\\\\Lenovo\\\\Ai_farmer_query_based\\\\data\", \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733e0fbe-fe4c-4b9f-88f8-720847e8610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query, top_k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append({\n",
    "            \"matched_question\": dataset[int(idx)][\"questions\"],\n",
    "            \"answer\": dataset[int(idx)][\"answers\"]\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce422891-f486-4c0f-ab8b-4e3a2040e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Best way to grow rice?\n",
      "Answer: The best time to plant rice depends on your region, but generally it is during the monsoon season between June and July.\n"
     ]
    }
   ],
   "source": [
    "# query = \"Best way to grow rice?\"\n",
    "# answer = get_answer(query)\n",
    "# print(\"Query:\", query)\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa027be-e2ef-4c8a-b058-67ec67ffa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Best fertilizer for wheat?\"\n",
    "answers = get_answer(query, top_k=3)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop 3 Retrieved Answers:\")\n",
    "for i, res in enumerate(answers, 1):\n",
    "    print(f\"\\n{i}. Q: {res['matched_question']}\\n   A: {res['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
